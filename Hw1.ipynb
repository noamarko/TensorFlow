{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbzw44FRH0sSmzyxkuXjqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noamarko/TensorFlow/blob/main/Hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRUD1_n3vQhK",
        "outputId": "89070aaf-70ed-4000-a919-aaaeb3798921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import numpy as np"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W7g4SJCX0MN"
      },
      "source": [
        "dim = 2  # Input dim\n",
        "out_dim = 1\n",
        "num_hidden = 4\n",
        "Temp = 0.001  # for sigmoid\n",
        "@tf.function\n",
        "def Linear_model2(x, w, b):\n",
        "  return tf.matmul(tf.cast(x, tf.float32), w) + b"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW5Kk7LcvUYK"
      },
      "source": [
        "@tf.function\n",
        "def BTU_layer(x, w, b):    \n",
        "  z1 = Linear_model2(x, w, b)  # [4, 2] X [dim, num_hidden] results in a vector [4, num_hidden] of z's. \n",
        "  # # Theoretically [1, num_hidden] for every input pair, which is the output of the hidden layer (before sigmoid).\n",
        "  return tf.sigmoid(z1/Temp)  # Element wise"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKLxmJsJxGaX"
      },
      "source": [
        "def Loss(x, w, b, n):\n",
        "\n",
        "  squared_deltas = tf.square(Linear_model2(x, w, b) - 0)\n",
        "  return tf.math.divide(tf.reduce_sum(squared_deltas), n, name = \"MSE\")"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU6L-hrux7Pw"
      },
      "source": [
        "def Hw1(n,k,bypass):\n",
        "  x = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "  \n",
        "  if k == 4:   \n",
        "        w1 = [[-1,-1,1,1],[-1,1,-1,1]]\n",
        "        b1 = [0.5, -0.5, -0.5, -1.5]\n",
        "        w2 = [[-1],[1],[1],[-1]]\n",
        "        b2 = 0.5\n",
        "  else:\n",
        "    if k == 2:\n",
        "        w1 = [[1,-1],[-1,1]]\n",
        "        b1 = [-0.5, -0.5]\n",
        "        w2 = [[2],[2]]\n",
        "        b2 = -0.5\n",
        "    else:\n",
        "        w1 = [[1],[1]]\n",
        "        b1 = [-1.5]\n",
        "        w2 = [[-2]]\n",
        "        b2 = -0.5\n",
        "\n",
        "  Hidden_Neurons(x, w1, b1, w2, b2, bypass, k)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSI9JoqC680Y"
      },
      "source": [
        "def Hidden_Neurons(x_train, w1, b1, w2, b2, bypass, k):\n",
        "  w1 = tf.Variable(w1, dtype=tf.float32, name=\"weights1\") #creating the weights variable\n",
        "  b1 = tf.Variable(b1, dtype=tf.float32, name=\"Biases1\") #creating the biase variable\n",
        "  tf.print(\"w1: \", w1)\n",
        "  tf.print(\"b1: \", b1)\n",
        "  hlayer = BTU_layer(x_train, w1, b1) #calculating the hidden layer (between w1 and w2)\n",
        "  tf.print(\"hidden layer: \", hlayer)\n",
        "  w2 = tf.Variable(w2, dtype=tf.float32, name=\"weights2\") #creating the second weights variable\n",
        "  tf.print(\"w2: \", w2)\n",
        "  if bypass == '1':\n",
        "    w_concat = Create_Weight_Concat(w2, k)\n",
        "    print(\"w_concat: \", w_concat)\n",
        "    b_concat = tf.Variable(b2, dtype=tf.float32, name=\"Biases1\")\n",
        "    print(\"b_concat: \", b_concat)\n",
        "    hlayer_concat = tf.concat([hlayer, x_train], axis=1)\n",
        "    print(\"hlayer_concat: \", hlayer_concat)\n",
        "    out_concat = BTU_layer(hlayer_concat, w_concat, b_concat)\n",
        "    tf.print(\"out_concat: \", out_concat)\n",
        "    mse = Loss(hlayer_concat,w_concat,b_concat,4)\n",
        "    tf.print(\"MSE:   \",mse)\n",
        "  else:\n",
        "    b2 = tf.Variable(b2, dtype=tf.float32, name=\"biase2\") #creating the second biase  \n",
        "    out = BTU_layer(hlayer, w2, b2) #calculating the final output \n",
        "    tf.print(\"output:    \", out)\n",
        "    mse = Loss(hlayer,w2,b2,4)\n",
        "    tf.print(\"MSE:   \",mse)"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPr4LnObKusS"
      },
      "source": [
        "def Create_Weight_Concat(w2, k):\n",
        "  if k == 4:\n",
        "    w_concat = tf.Variable([[-1], [1]], dtype=tf.float32, name=\"weights2\")\n",
        "    return tf.concat([w2,w_concat], axis=0)\n",
        "  else:\n",
        "    if k == 2:\n",
        "      w_concat = tf.Variable([[1], [-1]], dtype=tf.float32, name=\"weights2\")\n",
        "      return tf.concat([w2,w_concat], axis=0)\n",
        "    else:\n",
        "      w_concat = tf.Variable([[1], [1]], dtype=tf.float32, name=\"weights2\")\n",
        "      return tf.concat([w2,w_concat], axis=0)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2odHrdNN6GQI",
        "outputId": "74d3c9d6-048c-432d-ee82-93cb3042d1e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n = 2\n",
        "k = int(input(\"Number of hidden neuron: \"))\n",
        "bypass = input(\"Would you like to use a bypass? Press 1 for YES 0 for NO (NOTE: 1 hidden neuron has to use a bypass)\")\n",
        "Hw1(n, k, bypass)\n"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of hidden neuron: 2\n",
            "Would you like to use a bypass? Press 1 for YES 0 for NO (NOTE: 1 hidden neuron has to use a bypass)1\n",
            "w1:  [[1 -1]\n",
            " [-1 1]]\n",
            "b1:  [-0.5 -0.5]\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function BTU_layer at 0x7fb5f8a47ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "hidden layer:  [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 0]]\n",
            "w2:  [[2]\n",
            " [2]]\n",
            "w_concat:  tf.Tensor(\n",
            "[[ 2.]\n",
            " [ 2.]\n",
            " [ 1.]\n",
            " [-1.]], shape=(4, 1), dtype=float32)\n",
            "b_concat:  <tf.Variable 'Biases1:0' shape=() dtype=float32, numpy=-0.5>\n",
            "hlayer_concat:  tf.Tensor(\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 1. 0. 1.]\n",
            " [1. 0. 1. 0.]\n",
            " [0. 0. 1. 1.]], shape=(4, 4), dtype=float32)\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function BTU_layer at 0x7fb5f8a47ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "out_concat:  [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "MSE:    1.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpP67PnvU-7"
      },
      "source": [
        ""
      ]
    }
  ]
}